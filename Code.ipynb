{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec84dba",
   "metadata": {},
   "source": [
    "## Next Word Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31cbd158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef4d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216a4f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('A room with a view.txt','r',encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965ce05b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of A Room With A View by E M Forster This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever You may copy it give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www gutenberg org If you are not located in the United States you will have to check the laws of the country where you are located before using this eBo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = []\n",
    "for line in file:\n",
    "    lines.append(line)\n",
    "    \n",
    "content = \"\"\n",
    "for line in lines:\n",
    "    content = ' '.join(lines)\n",
    "    \n",
    "content = content.replace('\\n',' ').replace(\"\\r\",' ').replace('\"',' ').replace(\"'\",' ').replace(\"\\ufeff\",' ').replace(\",\",' ').replace(\".\",' ')\n",
    "\n",
    "content = content.split()\n",
    "content = \" \".join(content)\n",
    "content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2ef2b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382782"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55e2aeab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 113, 105, 594, 5]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([content])\n",
    "\n",
    "# saving tokenizer for predict func\n",
    "pickle.dump(tokenizer,open('token.pkl','wb'))\n",
    "\n",
    "seq_data = tokenizer.texts_to_sequences([content])[0]\n",
    "seq_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60d8f9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72076"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is less than the len(content) because in content, there are repeated words, and tokenizer converts each unique words into one numeric representation\n",
    "len(seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef108a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8213\n"
     ]
    }
   ],
   "source": [
    "# we have only this unique words\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45429efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 113, 105, 594],\n",
       "       [113, 105, 594,   5],\n",
       "       [105, 594,   5,   6],\n",
       "       [594,   5,   6, 114],\n",
       "       [  5,   6, 114,  19]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "for i in range(3, len(seq_data)):\n",
    "    words = seq_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "sequences = np.array(sequences)\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc54e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    x.append(i[:3])\n",
    "    y.append(i[-1])\n",
    "    \n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d0738e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting class vectors into binary class matrix\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633f221",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "Defining neural network and creating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19ede4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,10,input_length=3))   # size of vocab in my textdata, o/p dimension(size of vector space in which words will embedded) \n",
    "model.add(LSTM(1000,return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation = \"relu\"))          # converts -ve values to 0 and do nothing with +ve\n",
    "model.add(Dense(vocab_size, activation = \"softmax\")) # scales number into probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7af686e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 3, 10)             82130     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 8213)              8221213   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,352,343\n",
      "Trainable params: 21,352,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d511a36",
   "metadata": {},
   "source": [
    "## Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5949b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "keras.utils.plot_model(model, to_file='plot_image.png', show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9231e32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 6.7508\n",
      "Epoch 1: loss improved from inf to 6.75077, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 572s 497ms/step - loss: 6.7508\n",
      "Epoch 2/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 6.2242\n",
      "Epoch 2: loss improved from 6.75077 to 6.22421, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 1422s 1s/step - loss: 6.2242\n",
      "Epoch 3/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 5.8141\n",
      "Epoch 3: loss improved from 6.22421 to 5.81406, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 665s 590ms/step - loss: 5.8141\n",
      "Epoch 4/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 5.4884\n",
      "Epoch 4: loss improved from 5.81406 to 5.48839, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 554s 492ms/step - loss: 5.4884\n",
      "Epoch 5/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 5.2178\n",
      "Epoch 5: loss improved from 5.48839 to 5.21782, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 551s 489ms/step - loss: 5.2178\n",
      "Epoch 6/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 4.9743\n",
      "Epoch 6: loss improved from 5.21782 to 4.97427, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 1023s 908ms/step - loss: 4.9743\n",
      "Epoch 7/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 4.7411\n",
      "Epoch 7: loss improved from 4.97427 to 4.74115, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 2793s 2s/step - loss: 4.7411\n",
      "Epoch 8/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 4.4951\n",
      "Epoch 8: loss improved from 4.74115 to 4.49505, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 496s 440ms/step - loss: 4.4951\n",
      "Epoch 9/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 4.2394\n",
      "Epoch 9: loss improved from 4.49505 to 4.23940, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 420s 373ms/step - loss: 4.2394\n",
      "Epoch 10/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 3.9699\n",
      "Epoch 10: loss improved from 4.23940 to 3.96987, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 366s 325ms/step - loss: 3.9699\n",
      "Epoch 11/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 3.7027\n",
      "Epoch 11: loss improved from 3.96987 to 3.70272, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 369s 328ms/step - loss: 3.7027\n",
      "Epoch 12/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 3.4483\n",
      "Epoch 12: loss improved from 3.70272 to 3.44827, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 391s 347ms/step - loss: 3.4483\n",
      "Epoch 13/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 3.2097\n",
      "Epoch 13: loss improved from 3.44827 to 3.20972, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 395s 350ms/step - loss: 3.2097\n",
      "Epoch 14/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.9872\n",
      "Epoch 14: loss improved from 3.20972 to 2.98718, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 360s 319ms/step - loss: 2.9872\n",
      "Epoch 15/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.7707\n",
      "Epoch 15: loss improved from 2.98718 to 2.77068, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 361s 320ms/step - loss: 2.7707\n",
      "Epoch 16/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.5659\n",
      "Epoch 16: loss improved from 2.77068 to 2.56590, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 356s 316ms/step - loss: 2.5659\n",
      "Epoch 17/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.3645\n",
      "Epoch 17: loss improved from 2.56590 to 2.36454, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 383s 340ms/step - loss: 2.3645\n",
      "Epoch 18/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 2.1612\n",
      "Epoch 18: loss improved from 2.36454 to 2.16115, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 1748s 2s/step - loss: 2.1612\n",
      "Epoch 19/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.9645\n",
      "Epoch 19: loss improved from 2.16115 to 1.96451, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 2876s 3s/step - loss: 1.9645\n",
      "Epoch 20/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.7713\n",
      "Epoch 20: loss improved from 1.96451 to 1.77132, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 552s 490ms/step - loss: 1.7713\n",
      "Epoch 21/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.5808\n",
      "Epoch 21: loss improved from 1.77132 to 1.58080, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 558s 495ms/step - loss: 1.5808\n",
      "Epoch 22/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.4009\n",
      "Epoch 22: loss improved from 1.58080 to 1.40091, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 547s 486ms/step - loss: 1.4009\n",
      "Epoch 23/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.2351\n",
      "Epoch 23: loss improved from 1.40091 to 1.23513, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 547s 486ms/step - loss: 1.2351\n",
      "Epoch 24/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 1.0976\n",
      "Epoch 24: loss improved from 1.23513 to 1.09760, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 549s 487ms/step - loss: 1.0976\n",
      "Epoch 25/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.9643\n",
      "Epoch 25: loss improved from 1.09760 to 0.96430, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 550s 488ms/step - loss: 0.9643\n",
      "Epoch 26/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.8637\n",
      "Epoch 26: loss improved from 0.96430 to 0.86369, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 547s 485ms/step - loss: 0.8637\n",
      "Epoch 27/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.7750\n",
      "Epoch 27: loss improved from 0.86369 to 0.77497, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 1719s 2s/step - loss: 0.7750\n",
      "Epoch 28/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.7111\n",
      "Epoch 28: loss improved from 0.77497 to 0.71114, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 483s 429ms/step - loss: 0.7111\n",
      "Epoch 29/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.6514\n",
      "Epoch 29: loss improved from 0.71114 to 0.65145, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 361s 321ms/step - loss: 0.6514\n",
      "Epoch 30/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.6064\n",
      "Epoch 30: loss improved from 0.65145 to 0.60645, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 365s 324ms/step - loss: 0.6064\n",
      "Epoch 31/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.5780\n",
      "Epoch 31: loss improved from 0.60645 to 0.57801, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 370s 328ms/step - loss: 0.5780\n",
      "Epoch 32/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.5448\n",
      "Epoch 32: loss improved from 0.57801 to 0.54484, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 371s 329ms/step - loss: 0.5448\n",
      "Epoch 33/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.5212\n",
      "Epoch 33: loss improved from 0.54484 to 0.52123, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 402s 357ms/step - loss: 0.5212\n",
      "Epoch 34/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.5042\n",
      "Epoch 34: loss improved from 0.52123 to 0.50424, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 389s 345ms/step - loss: 0.5042\n",
      "Epoch 35/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4841\n",
      "Epoch 35: loss improved from 0.50424 to 0.48412, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 543s 482ms/step - loss: 0.4841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4645\n",
      "Epoch 36: loss improved from 0.48412 to 0.46448, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 348s 309ms/step - loss: 0.4645\n",
      "Epoch 37/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4470\n",
      "Epoch 37: loss improved from 0.46448 to 0.44699, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 470s 417ms/step - loss: 0.4470\n",
      "Epoch 38/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4451\n",
      "Epoch 38: loss improved from 0.44699 to 0.44505, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 382s 339ms/step - loss: 0.4451\n",
      "Epoch 39/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4249\n",
      "Epoch 39: loss improved from 0.44505 to 0.42491, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 391s 347ms/step - loss: 0.4249\n",
      "Epoch 40/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4154\n",
      "Epoch 40: loss improved from 0.42491 to 0.41544, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 378s 336ms/step - loss: 0.4154\n",
      "Epoch 41/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.4103\n",
      "Epoch 41: loss improved from 0.41544 to 0.41035, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 588s 522ms/step - loss: 0.4103\n",
      "Epoch 42/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3991 \n",
      "Epoch 42: loss improved from 0.41035 to 0.39915, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 32347s 29s/step - loss: 0.3991\n",
      "Epoch 43/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3955\n",
      "Epoch 43: loss improved from 0.39915 to 0.39555, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 926s 822ms/step - loss: 0.3955\n",
      "Epoch 44/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3853\n",
      "Epoch 44: loss improved from 0.39555 to 0.38525, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 2927s 3s/step - loss: 0.3853\n",
      "Epoch 45/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3763\n",
      "Epoch 45: loss improved from 0.38525 to 0.37632, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 1474s 1s/step - loss: 0.3763\n",
      "Epoch 46/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3687\n",
      "Epoch 46: loss improved from 0.37632 to 0.36869, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 2422s 2s/step - loss: 0.3687\n",
      "Epoch 47/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3672\n",
      "Epoch 47: loss improved from 0.36869 to 0.36718, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 544s 482ms/step - loss: 0.3672\n",
      "Epoch 48/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3591\n",
      "Epoch 48: loss improved from 0.36718 to 0.35910, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 504s 447ms/step - loss: 0.3591\n",
      "Epoch 49/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3555\n",
      "Epoch 49: loss improved from 0.35910 to 0.35555, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 941s 835ms/step - loss: 0.3555\n",
      "Epoch 50/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3542\n",
      "Epoch 50: loss improved from 0.35555 to 0.35417, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 404s 358ms/step - loss: 0.3542\n",
      "Epoch 51/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3442\n",
      "Epoch 51: loss improved from 0.35417 to 0.34423, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 380s 338ms/step - loss: 0.3442\n",
      "Epoch 52/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3371\n",
      "Epoch 52: loss improved from 0.34423 to 0.33709, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 379s 336ms/step - loss: 0.3371\n",
      "Epoch 53/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3368\n",
      "Epoch 53: loss improved from 0.33709 to 0.33677, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 380s 337ms/step - loss: 0.3368\n",
      "Epoch 54/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3287\n",
      "Epoch 54: loss improved from 0.33677 to 0.32865, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 380s 337ms/step - loss: 0.3287\n",
      "Epoch 55/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3295\n",
      "Epoch 55: loss did not improve from 0.32865\n",
      "1127/1127 [==============================] - 371s 329ms/step - loss: 0.3295\n",
      "Epoch 56/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3278\n",
      "Epoch 56: loss improved from 0.32865 to 0.32779, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 367s 325ms/step - loss: 0.3278\n",
      "Epoch 57/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3204\n",
      "Epoch 57: loss improved from 0.32779 to 0.32042, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 375s 333ms/step - loss: 0.3204\n",
      "Epoch 58/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3158\n",
      "Epoch 58: loss improved from 0.32042 to 0.31579, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 379s 336ms/step - loss: 0.3158\n",
      "Epoch 59/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3161\n",
      "Epoch 59: loss did not improve from 0.31579\n",
      "1127/1127 [==============================] - 380s 337ms/step - loss: 0.3161\n",
      "Epoch 60/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3155\n",
      "Epoch 60: loss improved from 0.31579 to 0.31547, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 379s 337ms/step - loss: 0.3155\n",
      "Epoch 61/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3042\n",
      "Epoch 61: loss improved from 0.31547 to 0.30425, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 379s 336ms/step - loss: 0.3042\n",
      "Epoch 62/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3037\n",
      "Epoch 62: loss improved from 0.30425 to 0.30370, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 374s 332ms/step - loss: 0.3037\n",
      "Epoch 63/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.2986\n",
      "Epoch 63: loss improved from 0.30370 to 0.29856, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 380s 337ms/step - loss: 0.2986\n",
      "Epoch 64/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.2975\n",
      "Epoch 64: loss improved from 0.29856 to 0.29749, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 427s 379ms/step - loss: 0.2975\n",
      "Epoch 65/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3052\n",
      "Epoch 65: loss did not improve from 0.29749\n",
      "1127/1127 [==============================] - 532s 472ms/step - loss: 0.3052\n",
      "Epoch 66/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.3023\n",
      "Epoch 66: loss did not improve from 0.29749\n",
      "1127/1127 [==============================] - 534s 474ms/step - loss: 0.3023\n",
      "Epoch 67/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.2893\n",
      "Epoch 67: loss improved from 0.29749 to 0.28935, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 536s 476ms/step - loss: 0.2893\n",
      "Epoch 68/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.2903\n",
      "Epoch 68: loss did not improve from 0.28935\n",
      "1127/1127 [==============================] - 536s 475ms/step - loss: 0.2903\n",
      "Epoch 69/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.2857\n",
      "Epoch 69: loss improved from 0.28935 to 0.28569, saving model to next_words.h5\n",
      "1127/1127 [==============================] - 535s 475ms/step - loss: 0.2857\n",
      "Epoch 70/70\n",
      "1127/1127 [==============================] - ETA: 0s - loss: 0.2901\n",
      "Epoch 70: loss did not improve from 0.28569\n",
      "1127/1127 [==============================] - 514s 456ms/step - loss: 0.2901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x265941b21f0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer=Adam(learning_rate = 0.001))\n",
    "model.fit(x,y, epochs=70, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1792200a",
   "metadata": {},
   "source": [
    "## Prediction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c97dfb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load model and tokenizer which we have saved\n",
    "model = load_model(\"next_words.h5\")\n",
    "tokenizer = pickle.load(open('token.pkl','rb'))\n",
    "\n",
    "def predict_next_word(model,tokenizer,text):\n",
    "    sequence_data = tokenizer.texts_to_sequences([text])\n",
    "    sequence_data = np.array(sequence_data)\n",
    "    prediction = np.argmax(model.predict(sequence_data))  # returns the indices of maximum value\n",
    "    predicted_word = \"\"\n",
    "    \n",
    "    for key, value in tokenizer.word_index.items():\n",
    "        if value == prediction:\n",
    "            predicted_word = key\n",
    "            break\n",
    "    print(predicted_word)\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46266c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text: License included with\n",
      "['License', 'included', 'with']\n",
      "1/1 [==============================] - 2s 2s/step\n",
      "this\n",
      "Enter text: Laureate that hung behind the\n",
      "['hung', 'behind', 'the']\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "english\n",
      "Enter text: 0\n",
      "Execution completed!\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    text = input(\"Enter text: \")\n",
    "    \n",
    "    if(text=='0'):\n",
    "        print(\"Execution completed!\")\n",
    "        break\n",
    "    else:\n",
    "        try:\n",
    "            text = text.split(\" \")\n",
    "            text = text[-3:]\n",
    "            print(text)\n",
    "            predict_next_word(model,tokenizer,text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(\"Error occurred\", e)\n",
    "            continue\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b5e252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
